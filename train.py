from env.maze_env import MazeEnv
from agent.dqn import Double_Agent
import torch
import time
import os

MODEL_PATH = "dqn_model.pth"

def save_model():
    torch.save({
        'model_state_dict': agent.online_net.state_dict(),
        'optimizer_state_dict': agent.optimizer.state_dict()
    }, MODEL_PATH)
    print(f"ðŸ’¾ Model saved to {MODEL_PATH}")

env = MazeEnv()
print("Successfully created environment env = MazeEnv()")
agent = Double_Agent(obs_dim=(3, 84, 84), act_dim=3)
print("If this message has not been reached agent was the problem")

# ðŸ”¹ Load model if available
if os.path.exists(MODEL_PATH):
    checkpoint = torch.load(MODEL_PATH)
    agent.online_net.load_state_dict(checkpoint['model_state_dict'])
    agent.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    print(f"âœ… Loaded model from {MODEL_PATH}")
else:
    print("â„¹ No saved model found â€” starting fresh.")

num_episodes = 500
action_interval = 0.2  # seconds
last_action_time = time.time()
current_action = None

try:
    for episode in range(num_episodes):
        agent.current_episode = episode
        print(f"\n[train.py] Starting episode {episode}")
        state = env.reset()
        total_reward = 0

        checkpoint_hit = False
        episode_step = 0

        # Pick first action immediately
        current_action = agent.act(state)
        last_action_time = time.time()

        for t in range(126):
            now = time.time()
            if now - last_action_time >= action_interval:
                if episode_step < 6 and not checkpoint_hit:
                    # EXPLOIT: Very low epsilon for first 6 steps
                    original_episode = agent.current_episode
                    agent.current_episode = 10000  # Force very low epsilon
                    current_action = agent.act(state)
                    agent.current_episode = original_episode  # Restore
                    if episode_step == 0:
                        print("ðŸŽ¯ EXPLOIT mode: Using learned strategy")
                else:
                        # EXPLORE: Normal epsilon decay
                    current_action = agent.act(state)
                    if episode_step == 6 and not checkpoint_hit:
                        print("ðŸ” EXPLORE mode: Normal exploration")
                
                last_action_time = now
                episode_step += 1

            next_state, reward, done, info = env.step(current_action)

            # ðŸ”¹ Occasionally print rewards during episode
            if t % 3 == 0:  # every 10 steps
                print(f"  Step {t} | Reward: {reward:.3f} | Total so far: {total_reward:.3f}")

            if done:
                reason = info.get("reason", "unknown")
                print(f"ðŸš« Episode {episode} ended due to {reason}! Final total reward: {total_reward:.3f}")
                input("Press ENTER to continue to next episode...")
                break

        save_model()
except KeyboardInterrupt:
    print("\nðŸ›‘ Training interrupted â€” saving model before exit...")
    save_model()

finally:
    env.close()
    print("Environment closed.")